# Processo_Seletivo_Observatorio

<h3> 1) Auto avaliação  </h3> 
<h4> Auto avalie suas habilidades nos requisitos de acordo com os níveis especificados. </h4>

<h4> Qual o seu nível de domínio nas técnicas/ferramentas listadas abaixo, onde:   </h4>

<p> ● 0, 1, 2 - não tem conhecimento e experiência; </p>
<p> ● 3, 4 ,5 - conhece a técnica e tem pouca experiência; </p>
<p> ● 6 - domina a técnica  e já desenvolveu vários projetos utilizando-a. </p>

<h4> Tópicos de Conhecimento: </h4>
<p> ● Ferramentas de visualização de dados (Power BI, Qlik Sense e outros): 6 </p>
<p> ● Manipulação e tratamento de dados com Python: 6 </p>
<p> ● Manipulação e tratamento de dados com Pyspark: 6 </p> 
<p> ● Desenvolvimento de data workflows em Ambiente Azure com databricks: 6 </p>
<p> ● Desenvolvimento de data workflows com Airflow: 6 </p>
<p> ● Manipulação de bases de dados NoSQL: 5 </p>
<p> ● Web crawling e web scraping para mineração de dados: 4 </p>
<p> ● Construção de APIs: REST, SOAP e Microservices: 2 </p>

<h3> 2) Desenvolvimento de pipelines de ETL de dados com Python, Apache Airflow e Spark </h3> 

<h4> a) Qual fonte de dados usar. </h4>

<p> Utilizando o modelo de dados Estatistíco Aquaviário - Antaq, disponibilizado no dicionário de dados, temos um banco altamente estruturado e normalizado, com isso o uso do SQL Server se tende a ser o mais propício, pois com ele podemos fazer análises complexas em SQL, e nossas consultas tendem a ter maior integridade. Também é mais indicada em cenários transacionais (OLPT) e análises que exijam uma consistência ACID. </p>

<h4> b) Scripts. </h4>

<p> Resposta no arquivo etl_spark_sqlserver.py </p>

<h4> b) Scripts PySpark criação tabelas atracao_fato e carga_fato. </h4>

<p> Resposta no arquivo etl_spark_sqlserver.py </p>

<h4> b) Scripts SQL consulta tabelas com dados Ceará, Nordeste e Brasil. </h4>

<p> Resposta no arquivo ConsultaNaviosAtracar.sql </p>

<h4> 3) Criação da DAG para base ANTAQ. </h4>

<p> Resposta no arquivo DAG_Etl_ANTAQ.py </p>
